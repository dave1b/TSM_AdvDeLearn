{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive Model for Explicit Density Estimation\n",
    "\n",
    "In this approach, we model the likelihood directly, making the computation tractable by leveraging the chain rule to decompose the probability of a pixel $x_i$ into a conditional probability. The context for $x_i$ is provided by the preceding pixels. This conditional information is captured by a recurrent neural network (RNN) through its hidden state, which is passed forward in the sequence. The hidden state then serves as the context for predicting the next pixel value.\n",
    "\n",
    "The probability of the data can be expressed as:\n",
    "$$\n",
    "p(x) = \\prod_{i=1}^n p\\left(x_i \\mid x_1, \\ldots, x_{i-1}\\right)\n",
    "$$\n",
    "\n",
    "To reduce computational demands, I reformulated the problem as a sequence prediction task. Instead of using $(n \\times m)$ images as instances, I generated sinusoidal patterns represented as vectors. These vectors have values constrained between 0 and 255 and a sequnce length of 16, simplifying the input space while maintaining the structure necessary for autoregressive modeling. Therefore instead of PixelRNN we are training a vanilla Long short-term memory (LSTM). These models are similar to RNNs but protect the integrity of the hidden state making the model more sensitive to long time series data. In this notebook you will complete code for a Dataset class, LSTM autoregressive class, and a training function. You can inspect your models performance visually simply by feeding the trained model and data instances into the given plotting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sinusoidal data\n",
    "def create_sinusoidal_dataset(num_samples=1000, vector_length=16, frequency=2):\n",
    "    x = np.linspace(0, 2 * np.pi * frequency, vector_length)\n",
    "    data = np.zeros((num_samples, vector_length), dtype=np.uint8)\n",
    "    for i in range(num_samples):\n",
    "        amplitude = np.random.uniform(50, 100)\n",
    "        phase = np.random.uniform(0, 2 * np.pi)\n",
    "        offset = np.random.uniform(100, 150)\n",
    "        pattern = amplitude * np.sin(x + phase) + offset\n",
    "        pattern = np.clip(pattern, 0, 255)\n",
    "        data[i] = pattern.astype(np.uint8)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random samples of the data\n",
    "data = create_sinusoidal_dataset()\n",
    "plt.figure(figsize=(18, 7))\n",
    "for i in range(5):\n",
    "    plt.subplot(5, 1, i + 1)\n",
    "    plt.plot(data[i], linewidth=1, color='blue')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom Dataset class that loads the sinusoidal data and returns input-target pairs of type torch.float32\n",
    "# The is the data generated by the create_sinusoidal_dataset function while the target is the same data shifted by one step\n",
    "class SinusoidalDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "\n",
    "        # Your code here\n",
    "\n",
    "    def __len__(self):\n",
    "            \n",
    "            # Your code here\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            \n",
    "            # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your SinusoidalDataset class by running this code block \n",
    "# # The block creates a dataset and dataloader using create_sinusoidal_dataset and the SinusoidalDataset class\n",
    "# The output of the block should print torch.Size([1, 15]) torch.Size([1, 15]), and 15 pairs of input-target sequences offset by one step\n",
    "data = create_sinusoidal_dataset(1, 16)\n",
    "train_dataset = SinusoidalDataset(data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1)\n",
    "x, target = next(iter(train_loader))\n",
    "print(x.shape, target.shape)\n",
    "for i in range(x.size(1)):\n",
    "    print(f'Input: {x[0, i].item():.2f} -> Target: {target[0, i].item():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "# The model should have an LSTM layer followed by a fully connected layer\n",
    "# The fully connected layer should output one value at a time (the next value in the sequence)\n",
    "# Remember to set batch_first=True in the LSTM layer\n",
    "# And that lstms return two values, the output and the hidden state \n",
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        \n",
    "        # Your code here\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Your code here\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 1])\n"
     ]
    }
   ],
   "source": [
    "# The code block below tests the LSTMPredictor model by passing a random input tensor of shape (1, 15, 1) through the model\n",
    "# The output shape should be (1, 15, 1)\n",
    "model = LSTMPredictor(input_size=1, hidden_size=16, num_layers=1)\n",
    "out = model(x.unsqueeze(-1)) # model takes input of shape (batch_size, seq_len, input_size)\n",
    "print(out.shape) # output shape: (batch_size, seq_len, 1) -> (1, 15, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that trains the model\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs, device):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Add extra dimension for single feature input\n",
    "            inputs = inputs.unsqueeze(-1)\n",
    "            targets = targets.unsqueeze(-1)\n",
    "\n",
    "            # Your code here\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters\n",
    "num_samples = 2000\n",
    "vector_length = 16\n",
    "batch_size = 32\n",
    "input_size = 1\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "num_epochs = 200\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and DataLoader\n",
    "data = create_sinusoidal_dataset(num_samples, vector_length)\n",
    "train_dataset = SinusoidalDataset(data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, loss function, optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMPredictor(input_size, hidden_size, num_layers).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use function as is to plot predictions and visualy inspect the model\n",
    "def plot_predictions_with_context_shading(data, model, context_length, num_samples, device, fig_size=(10, 8)):\n",
    "    \"\"\"\n",
    "    Plot original data and model predictions with grid background and shaded context region.\n",
    "\n",
    "    Parameters:\n",
    "        data (numpy array): Original dataset.\n",
    "        model (torch.nn.Module): Trained RNN model.\n",
    "        context_length (int): Length of context used for prediction.\n",
    "        num_samples (int): Number of samples to visualize.\n",
    "        device (torch.device): Device to use for inference.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(num_samples, 1, figsize=fig_size, sharex=True)\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    for i in range(num_samples):\n",
    "        original_sequence = data[i]\n",
    "        context_sequence = original_sequence[:context_length]\n",
    "        ground_truth = original_sequence[context_length:]\n",
    "        \n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            input_seq = torch.tensor(context_sequence, dtype=torch.float32).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "            for _ in range(len(ground_truth)):\n",
    "                output = model(input_seq)\n",
    "                next_val = output[:, -1, :].squeeze().cpu().item()\n",
    "                predictions.append(next_val)\n",
    "                # Update input sequence with the new prediction\n",
    "                next_input = torch.tensor([[next_val]], dtype=torch.float32).to(device)\n",
    "                input_seq = torch.cat((input_seq[:, 1:, :], next_input.unsqueeze(0)), dim=1)\n",
    "\n",
    "        # Plotting\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Add grid and shaded context region\n",
    "        ax.grid(True, linestyle='--', alpha=0.5)\n",
    "        ax.add_patch(Rectangle((0, ax.get_ylim()[0]), context_length, ax.get_ylim()[1] - ax.get_ylim()[0],\n",
    "                               color='lightgrey', alpha=0.3, zorder=0))\n",
    "        \n",
    "        # Plot ground truth, context, and predictions\n",
    "        ax.plot(range(len(original_sequence)), original_sequence, label=\"Ground Truth\", color=\"green\")\n",
    "        ax.plot(range(context_length), context_sequence, label=\"Context\", color=\"blue\")\n",
    "        ax.scatter(range(context_length, len(original_sequence)), predictions, color=\"red\", label=\"Predictions\")\n",
    "        \n",
    "        ax.legend(loc=\"upper left\")\n",
    "        ax.set_title(f\"Sample {i+1}\")\n",
    "        ax.set_xlabel(\"Time Steps\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models performance by plotting predictions\n",
    "plot_predictions_with_context_shading(data, model, context_length=8, num_samples=5, device=device, fig_size=(18, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens when you decrease the context length. Use the above plot function to make an argument. Explain your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    print(f\"Context Length: {i}\")\n",
    "    plot_predictions_with_context_shading(data, model, context_length=i, num_samples=1, device=device, fig_size=(19, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_adl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
